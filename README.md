## Multimodal RLHF: Enhancing AI Perception
This project implements a framework for fine-tuning multi-modal networks using Reinforcement Learning from Human Feedback (RLHF). I am trying to integrate llama-3 (text), Stable Diffusion (image), and NeRF (3D scene reconstruction), so that the system improves cross-modal consistency and the aim is to reduce sensor data hallucinations by 30%.
The work leverages 10K+ human preference annotations to create more reliable AI perception systems with applications in healthcare, autonomous vehicles, and assistive technologies. By addressing fundamental challenges in multimodal perception, I am aim to contribute through this project to the development of AI systems that are not only technically advanced but also more aligned with human expectations, ultimately serving societal well-being through safer and more reliable automation solutions.
